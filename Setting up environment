Week 1: Introduction to Tools and Frameworks
- Introduction to Linux, WSL, Git, and GitHub
- Setup development environment
- Familiarization with RASA and YOLO frameworks

Week 2: Fundamentals of Conversational AI

- Understanding the basics of conversational AI
- Exploring RASA architecture and components
- Setting up a simple chatbot with RASA

Week 3: Advanced RASA Concepts

- Customizing chatbot behavior with intents, entities, and actions
- Implementing slot filling and forms
- Introduction to RAG framework for conversational retrieval


WEEK 1 :- 
#for a MAC USER setting environment:_
1. install Tensorflow & Pytorch
2. set up pyhtonn virtual environmentwith code given
[[
source: https://www.youtube.com/watch?v=Y21OR1OPC9A
1. create an env   : python3 -m venv NAME
2. activate an env : source NAME/bin/activate
3.  install individual packages : in this we will share a package requirement to others so they can run our project on other environments : pip freeze > requirements.txt     this commmant will give a list of files needed for others to run our programe with their version.
4. install packages from a file: pip install -r requirments.txt  ( -r command helps to download packages mentioned in the reuirements.txt file)
5. save packages : 
6. to install packages: python3 -m pip install NAME
]]
# checking the environment setup 
import tensorflow as tf
import torch
import onnx
import onnxruntime 

def check_tensorflow_gpu():
    if tf.test.is_gpu_available():
        print("TensorFlow GPU support is available.")
        physical_devices = tf.config.list_physical_devices('GPU')
        for device in physical_devices:
            print(f"GPU Name: {device.name}")
    else:
        print("TensorFlow GPU not found. Using CPU.")

def check_pytorch_gpu():
    if torch.cuda.is_available():
        print("PyTorch GPU support is available.")
        gpu_name = torch.cuda.get_device_name(0)
        print(f"GPU Name: {gpu_name}")
    else:
        print("PyTorch GPU not found. Using CPU.")

def check_onnx_gpu():
    device = onnxruntime.get_device()
    print(device)
    if device == 'GPU':
        print("ONNX GPU support is available.")
        gpu_name = onnxruntime.get_available_providers()
        print(f"Available Execution Providers: {gpu_name}")
    else:
        print("ONNX GPU not found. Using CPU.")

if __name__ == "__main__":
    print("Checking TensorFlow GPU support:")
    check_tensorflow_gpu()

    print("\nChecking PyTorch GPU support:")
    check_pytorch_gpu()

    print("\nChecking ONNX GPU support:")
    check_onnx_gpu()

